{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Recognizing Hand written Digits with deep learning\"\n",
    "> \"I trained the model with models written from scratch as well as pre trained models\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/MNIST/mnist.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing FAST AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the list of files and directories in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/0'),Path('training/2'),Path('training/9'),Path('training/8'),Path('training/7'),Path('training/1'),Path('training/5'),Path('training/4'),Path('training/6'),Path('training/3')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the files from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes = (path/'training'/'0').ls().sorted()\n",
    "ones = (path/'training'/'1').ls().sorted()\n",
    "twos = (path/'training'/'2').ls().sorted()\n",
    "threes = (path/'training'/'3').ls().sorted()\n",
    "fours = (path/'training'/'4').ls().sorted()\n",
    "fives = (path/'training'/'5').ls().sorted()\n",
    "sixes = (path/'training'/'6').ls().sorted()\n",
    "sevens = (path/'training'/'7').ls().sorted()\n",
    "eights = (path/'training'/'8').ls().sorted()\n",
    "nines = (path/'training'/'9').ls().sorted()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoElEQVR4nGNgGASA98pfCxibCV2SW+N/ME6dOX/+6OKUfPnnMCcuOdO/f/zhHHQ77ZkYv+GUZPj36ApOK4//XYdTTund3xickhP+vODAJef29+9dXHKcs//8iccl6ffnzwpkPopXghkYHuLSWP/lzyUxXJKn/v5NwiUX//fPbkEIk1UQXfLZ31vCUCa7JJqcz4+/BbgMZeB8sQVn4NARAABosDHlWlHdewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F37A44B1820>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3_path = ones[1]\n",
    "im3 = Image.open(im3_path)\n",
    "im3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting each image to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_tensors = [tensor(Image.open(o)) for o in zeroes]\n",
    "one_tensors = [tensor(Image.open(o)) for o in ones]\n",
    "two_tensors = [tensor(Image.open(o)) for o in twos]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "four_tensors = [tensor(Image.open(o)) for o in fours]\n",
    "five_tensors = [tensor(Image.open(o)) for o in fives]\n",
    "six_tensors = [tensor(Image.open(o)) for o in sixes]\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "eight_tensors = [tensor(Image.open(o)) for o in eights]\n",
    "nine_tensors = [tensor(Image.open(o)) for o in nines]\n",
    "\n",
    "len(three_tensors),len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking each image on top of image to make it a stack of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ones = torch.stack(one_tensors).float()/255\n",
    "stacked_twos = torch.stack(two_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "stacked_fours = torch.stack(four_tensors).float()/255\n",
    "stacked_fives = torch.stack(five_tensors).float()/255\n",
    "stacked_sixes = torch.stack(six_tensors).float()/255\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_eights = torch.stack(eight_tensors).float()/255\n",
    "stacked_nines = torch.stack(nine_tensors).float()/255\n",
    "stacked_zeroes = torch.stack(zero_tensors).float()/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1009, 28, 28]), torch.Size([980, 28, 28]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_0_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'0').ls()])\n",
    "valid_0_tens = valid_0_tens.float()/255\n",
    "valid_1_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'1').ls()])\n",
    "valid_1_tens = valid_1_tens.float()/255\n",
    "valid_2_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'2').ls()])\n",
    "valid_2_tens = valid_2_tens.float()/255\n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "valid_4_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'4').ls()])\n",
    "valid_4_tens = valid_4_tens.float()/255\n",
    "valid_5_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'5').ls()])\n",
    "valid_5_tens = valid_5_tens.float()/255\n",
    "valid_6_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'6').ls()])\n",
    "valid_6_tens = valid_6_tens.float()/255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "valid_8_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'8').ls()])\n",
    "valid_8_tens = valid_8_tens.float()/255\n",
    "valid_9_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'9').ls()])\n",
    "valid_9_tens = valid_9_tens.float()/255\n",
    "valid_9_tens.shape,valid_0_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5421, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_fives.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our independent variables\n",
    "x\n",
    "â€”these are the images themselves. We'll concatenate them allinto a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2tensor). We can do this using\n",
    "view\n",
    ", which is a PyTorch method that changes the shape of a tensor withoutchanging its contents.\n",
    "-1\n",
    "is a special parameter to\n",
    "view\n",
    "that means \"make this axis as big as necessary tofit all the data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_zeroes,stacked_ones,stacked_twos,stacked_threes,stacked_fours,stacked_fives,\n",
    "                     stacked_sixes,stacked_sevens,stacked_eights,stacked_nines]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor([0]*len(zeroes) + [1]*len(ones)+[2]*len(twos)+[3]*len(threes)+[4]*len(fours)+[5]*len(fives)+\n",
    "                [6]*len(sixes)+[7]*len(sevens)+[8]*len(eights)+[9]*len(nines)).unsqueeze(1)\n",
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A\n",
    "Dataset\n",
    "in PyTorch is required to return a tuple of\n",
    "(x,y)\n",
    "when indexed. Python provides a\n",
    "zip\n",
    "functionwhich, when combined with\n",
    "list\n",
    ", provides a simple way to get this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = list(zip(train_x,train_y))\n",
    "x,y = dset[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([valid_0_tens,valid_1_tens,valid_2_tens,valid_3_tens,valid_4_tens,valid_5_tens,\n",
    "                     valid_6_tens,valid_7_tens,valid_8_tens,valid_9_tens]).view(-1, 28*28)\n",
    "valid_y = tensor([0]*len(valid_0_tens) + [1]*len(valid_1_tens)+[2]*len(valid_2_tens)+[3]*len(valid_3_tens)+\n",
    "                 [4]*len(valid_4_tens)+[5]*len(valid_5_tens)+\n",
    "                 [6]*len(valid_6_tens)+[7]*len(valid_7_tens)+[8]*len(valid_8_tens)+[9]*len(valid_9_tens)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sigmoid function for loss so that the values remain between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a batch to train data instead of using all images or one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear\n",
    "does the same thing as our\n",
    "init_params\n",
    "and\n",
    "linear\n",
    "together. It contains both the\n",
    "weights\n",
    "and\n",
    "biases\n",
    "in a single class. Here's how we replicate our model from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting our model with a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.105249</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.084339</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.066740</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.046343</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.040353</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.035921</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.032544</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.008706</td>\n",
       "      <td>0.029901</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.027785</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a simple neuronet by adding non linearity and adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.109438</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.092503</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.067543</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.060508</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.050902</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.047355</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.044345</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.041761</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.039507</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.037527</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.035778</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.031585</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.030461</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.029445</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.028522</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.025543</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.024367</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.023838</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.021642</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.021275</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.020928</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.020288</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.019193</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.018953</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(40,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19709999859333038"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see after appying neural net the accuracy is around 20% which is not satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that a single nonlinearity with two linear layers is enough to approximate any function. Sowhy would we use deeper models? The reason is performance. With a deeper model (that is, one with morelayers) we do not need to use as many parameters; it turns out that we can use smaller matrices with morelayers, and get better results than we would get with larger matrices, and few layers.\n",
    "That means that we can train the model more quickly, and it will take up less memory. In the 1990s researcherswere so focused on the universal approximation theorem that very few were experimenting with more than onenonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers,however, did experiment with deep models, and eventually were able to show that these models could performmuch better in practice. Eventually, theoretical results were developed which showed why this happens. Today,it is extremely unusual to find anybody using a neural network with just one nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 18 layers pre trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.data.core.DataLoaders at 0x7f36d07f8a00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = ImageDataLoaders.from_folder(path=path, train='training', test='testing', valid_pct=0.2);\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060049</td>\n",
       "      <td>0.039713</td>\n",
       "      <td>0.989357</td>\n",
       "      <td>08:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = cnn_learner(db, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now the accuracy jumps to 98.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
